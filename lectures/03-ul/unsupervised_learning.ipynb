{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to make plots look nice\n",
    "import pylab\n",
    "global_params = {\n",
    "    'text.usetex' : False,\n",
    "    'font.family' : 'serif',\n",
    "    'font.sans-serif' : ['Helvetica'],\n",
    "    'ps.usedistiller' : 'xpdf',\n",
    "    'ps.distiller.res' : 3000,\n",
    "    'axes.labelsize' : 13,\n",
    "          #'text.fontsize' : 16,\n",
    "    'legend.fontsize' : 10,\n",
    "    'xtick.labelsize' : 12,\n",
    "    'ytick.labelsize' : 12,\n",
    "    'axes.linewidth': 2.0,\n",
    "    'axes.grid':True,  \n",
    "    'figure.figsize' : [7,5],\n",
    "    'grid.linestyle':'--',\n",
    "    'grid.color':'k',\n",
    "    'grid.alpha':0.1}\n",
    "pylab.rcParams.update(global_params) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will explore how different unsupervised learning algorithms operate on the same data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "data=pd.read_csv('Unsupervised.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data comes from a subset of the Bright Transient Survey (BTS) data set [cite this better]. There are only 2 classes, 'SN Ia' and 'SN II' which can be found under the 'type' column. For the guided part of the notebook we will be looking at the peak absolute magnitude ('peakabs') and fade ('fade') features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Create a pandas dataframe that only contains the columns 'ZTFID' 'peakabs' 'fade' and 'type'\n",
    "relavant_data=..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been pared down we can explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Make a scatter plot showing each type separately\n",
    "fig,ax=plt.subplots(figsize=(8,6))\n",
    "SN1a_data=...\n",
    "SN2_data=...\n",
    "ax.scatter(....)\n",
    "ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye we can see that the two classes live in different areas of this plane. SNII tend to have longer fade time and are dimmer (larger peak magnitude). Also SN II have a larger spread compared to SN Ia. If you do not see these trends ask for help from the instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a dive into the unsupervised learning algorithms! The first thing we should do is scale the data. Let's use the standard scaler provided by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# T0 do: apply the standard scaler to the data\n",
    "# see docs at : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "feature_data=... #get the data for only the features and cast them to floats\n",
    "scaler=StandardScaler().fit(feature_data)\n",
    "scaled_data=scaler.transform(feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.k_means.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kmeans algorithms main knob to turn is the number of cluster. Lets see what it looks like to use different number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: train kemans for 2,5 and 10 clusters\n",
    "\n",
    "#example\n",
    "ex_model=KMeans(n_clusters=15) #define a model\n",
    "ex_model.fit(scaled_data)      #fit the model\n",
    "ex_model.predict(scaled_data)  #get the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot them and compare to the true classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Plot the true classes and 3 kmeans\n",
    "#hint: use Kmeans.fit(scaled_data).predict(scaled_data) for your color map\n",
    "fig,axs=plt.subplots(2,2,figsize=(8,6))\n",
    "axs=axs.flatten()\n",
    "axs[0].scatter(SN1a_data['peakabs'],SN1a_data['fade'],label='SN Ia',s=5)\n",
    "axs[0].scatter(SN2_data['peakabs'],SN2_data['fade'],label='SN II',s=5)\n",
    "axs[0].set_title('True Classes')\n",
    "axs[0].legend()\n",
    "n_clus=[...]\n",
    "for i,model in enumerate(n_clus):\n",
    "    axs[i+1].scatter(feature_data['peakabs'],feature_data['fade'],c=...)\n",
    "    axs[i+1].set_title(f'N_cluster={n_clus[i]}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look? Can we determine the number of clusters systematically? KMeans is not well equipped to answer this question, keep this in mind for Gaussian Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN does not take a number of clusters as a parameter. Instead it has epsilon and min samples as tunable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Test a few different epsilon values and find out how many classes are created\n",
    "fig,axs=plt.subplots(2,2,figsize=(8,6))\n",
    "axs=axs.flatten()\n",
    "axs[0].scatter(SN1a_data['peakabs'],SN1a_data['fade'],label='SN Ia',s=5)\n",
    "axs[0].scatter(SN2_data['peakabs'],SN2_data['fade'],label='SN II',s=5)\n",
    "axs[0].set_title('True Classes')\n",
    "axs[0].legend()\n",
    "\n",
    "epsilons=[...] #pick 3 values here\n",
    "\n",
    "for i,eps in enumerate(epsilons):\n",
    "    model=DBSCAN(...) #keep min_samples as the default\n",
    "    model.fit(...)\n",
    "    axs[i].scatter(feature_data['peakabs'],feature_data['fade'],c=model.predict(scaled_data),s=5)\n",
    "    axs[i].set_title(f'epsilon= {eps}. Number of classes= {...}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Now do the same but vary min samples. Keep eps=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing epsilon and minimum samples can dramaticly change the outcome. Now lets do some bad science and try to pick epsilon and minimum samples to match our data! (this is for academic purposes only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the density of the two classes differ DBSCAN is not ideal for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage that Gaussian Mixture Model (GMM) has is its statistical basis. This implementaion of GMM has built in functions for the Bayesian and Akaike information criterion (BIC and AIC). These give you an idea of how good the model is. We can use this to determine how many clusters we should choose!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: Compute and Plot the BIC and AIC for a variety of cluster numbers\n",
    "number_of_clusters=[1,2,3,4,5,6,7,8,9,10]\n",
    "bic=[]\n",
    "aic=[]\n",
    "#example\n",
    "example_model=GaussianMixture(n_components=1).fit(scaled_data)\n",
    "ex_bic=example_model.bic(scaled_data)\n",
    "ex_aic=example_model.aic(scaled_data)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.plot(number_of_clusters,bic,label='BIC',marker='o')\n",
    "ax.plot(number_of_clusters,aic,label='AIC',marker='o')\n",
    "ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot we can see that the BIC is minimum at 3 and the AIC starts to get diminishing returns at 3. So this tells us that the number of cluster is probably 2,3 or 4.(If you don't see this ask for help) Now let's fit for those parameters and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Plot gmm models for 2,3 and 4 clusters.\n",
    "fig,axs=plt.subplots(2,2,figsize=(8,6))\n",
    "axs=axs.flatten()\n",
    "axs[0].scatter(SN1a_data['peakabs'],SN1a_data['fade'],label='SN Ia',s=5)\n",
    "axs[0].scatter(SN2_data['peakabs'],SN2_data['fade'],label='SN II',s=5)\n",
    "axs[0].set_title('True Classes')\n",
    "axs[0].legend()\n",
    "\n",
    "number_of_clusters=[...] \n",
    "\n",
    "for i,n_clus in enumerate(number_of_clusters):\n",
    "    model=GaussianMixture(n_components=...)\n",
    "    model.fit(...)\n",
    "    axs[i+1].scatter(feature_data['peakabs'],feature_data['fade'],c=model.predict(scaled_data),s=5)\n",
    "    axs[i+1].set_title(f' Number of clusters= {n_clus}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 clusters doesn't look too bad! Now let's try to extract the gaussians from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot a contour\n",
    "def plot_contour(ax,center,cov,lvls=3,cmap='viridis'):\n",
    "    '''\n",
    "    ax:     matplotlib axis that you want the contour to be plotted on\n",
    "    center: array of mean of gaussian [x_mean,ymean]\n",
    "    cov:    covariance matrix\n",
    "    sig:    int, number of contours to plot\n",
    "    cmap:   Matplotlib color map\n",
    "    '''\n",
    "    x = np.arange(-21, -14, .1)\n",
    "    y = np.arange(0, 120, .1)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    xhat=center[0]\n",
    "    yhat=center[1]\n",
    "    sigx=cov[0][0]\n",
    "    sigy=cov[1][1]\n",
    "    A=(X-xhat)**2/((2*sigx)**2) #Ignoring covariance\n",
    "    B=(Y-yhat)**2/(2*sigy**2)\n",
    "    Z=np.exp(-(A+B))\n",
    "    ax.contour(X, Y, Z,levels=lvls-1,linestyles='dashed',cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Plot the contours of the gaussians found by the model over the data\n",
    "fig,ax=plt.subplots(figsize=(8,6))\n",
    "model=GaussianMixture(n_components=...).fit(feature_data) #Use feature data instead of scaled data \n",
    "mean=...\n",
    "cov=...\n",
    "ax.scatter(SN1a_data['peakabs'],SN1a_data['fade'],label='SN Ia',s=5)\n",
    "ax.scatter(SN2_data['peakabs'],SN2_data['fade'],label='SN II',s=5)\n",
    "ax.set_title('True Classes')\n",
    "plot_contour(...)\n",
    "plot_contour(...)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plot showing all 3 algorithms at once. I hope you all have learned when each one can be used as well as the advantages and disadvantages of each!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,2,figsize=(8,6))\n",
    "axs=axs.flatten()\n",
    "axs[0].scatter(SN1a_data['peakabs'],SN1a_data['fade'],label='SN Ia',s=5)\n",
    "axs[0].scatter(SN2_data['peakabs'],SN2_data['fade'],label='SN II',s=5)\n",
    "axs[0].set_title('True Classes')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "kmeans_model=KMeans(n_clusters=2).fit(scaled_data)\n",
    "axs[1].scatter(feature_data['peakabs'],feature_data['fade'],c=kmeans_model.predict(scaled_data),s=5)\n",
    "axs[1].set_title('Kmeans')\n",
    "\n",
    "\n",
    "\n",
    "gmm_model=GaussianMixture(n_components=2).fit(scaled_data)\n",
    "axs[2].scatter(feature_data['peakabs'],feature_data['fade'],c=gmm_model.predict(scaled_data),s=5)\n",
    "axs[2].set_title('GMM')\n",
    "\n",
    "db=DBSCAN(eps=0.15,min_samples=7).fit(scaled_data)\n",
    "axs[3].scatter(feature_data['peakabs'],feature_data['fade'],c=db.labels_,s=5,cmap='turbo')\n",
    "axs[3].set_title('DBSCAN')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "Try your hand at these 2 things! \n",
    "1) Compute an accuracy for the various methods. Typically, you wouldn't know the classes in an unsupervised problem but we do here. See how they performed! \n",
    "2) Use different \"important\" features and see what they look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('ztf_summer_school')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5685a55b899b8570f2ef2a994c27552f322d1b7acec4e7681beb8e2bc9df865"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
